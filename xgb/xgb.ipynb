{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc872512",
   "metadata": {},
   "source": [
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311435ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import modin.pandas as md\n",
    "import snowflake.snowpark.modin.plugin\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "from xgb_helpers import (\n",
    "    split_numeric_categorical,\n",
    "    build_dmatrix,\n",
    "    OrdinalEncoder,\n",
    "    NumericImputerArbitrary,\n",
    "    RichProgressBarCallback,\n",
    ")\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "np.int = int\n",
    "session = get_active_session()\n",
    "session.use_schema(\"ML_AUTOMATION\")\n",
    "\n",
    "CONFIG_FILE = \"config.ini\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_FILE)\n",
    "\n",
    "\n",
    "def _normalize_config_entry(value: Optional[str]) -> Optional[str]:\n",
    "    if value is None:\n",
    "        return None\n",
    "    stripped = value.strip()\n",
    "    if stripped.lower() in {\"\", \"none\"}:\n",
    "        return None\n",
    "    return stripped\n",
    "\n",
    "\n",
    "def _parse_value(value: str):\n",
    "    lower = value.strip().lower()\n",
    "    if lower in {\"true\", \"false\"}:\n",
    "        return lower == \"true\"\n",
    "    try:\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            return value\n",
    "\n",
    "\n",
    "train_path = _normalize_config_entry(config[\"PATHS\"].get(\"train\"))\n",
    "test_path = _normalize_config_entry(config[\"PATHS\"].get(\"test\"))\n",
    "\n",
    "if not train_path or not test_path:\n",
    "    raise ValueError(\"Both train and test Snowflake objects must be defined in config.ini\")\n",
    "\n",
    "df_train = md.read_snowflake(name_or_query=train_path)\n",
    "df_test = md.read_snowflake(name_or_query=test_path)\n",
    "\n",
    "target_column = config[\"VARIABLES\"].get(\"y_col\", \"\")\n",
    "identifier_str = config[\"VARIABLES\"].get(\"identifier_cols\", \"\")\n",
    "identifier_cols = [col.strip() for col in identifier_str.split(\",\") if col.strip()]\n",
    "modelling_features = [\n",
    "    col.strip()\n",
    "    for col in config[\"VARIABLES\"].get(\"modelling_features\", \"\").split(\",\")\n",
    "    if col.strip()\n",
    "]\n",
    "\n",
    "if not target_column:\n",
    "    raise ValueError(\"A target column must be provided via [VARIABLES] y_col in config.ini\")\n",
    "\n",
    "# Convert to pandas early to avoid relying on private Modin APIs\n",
    "df_train = df_train.to_pandas()\n",
    "df_test = df_test.to_pandas()\n",
    "\n",
    "if target_column not in df_train.columns:\n",
    "    raise ValueError(f\"Target column '{target_column}' is not present in the training dataset\")\n",
    "\n",
    "if not modelling_features:\n",
    "    raise ValueError(\n",
    "        \"No modelling features were provided via [VARIABLES] modelling_features in config.ini\"\n",
    "    )\n",
    "\n",
    "identifier_cols_in_test = [c for c in identifier_cols if c in df_test.columns]\n",
    "df_test_identifiers = df_test.loc[:, identifier_cols_in_test].copy()\n",
    "\n",
    "valid_features = [feature for feature in modelling_features if feature in df_train.columns]\n",
    "if target_column in valid_features:\n",
    "    valid_features.remove(target_column)\n",
    "\n",
    "if not valid_features:\n",
    "    raise ValueError(\"None of the configured modelling features are present in the training set\")\n",
    "\n",
    "y = df_train[target_column].copy()\n",
    "X_train = df_train.loc[:, valid_features].copy()\n",
    "\n",
    "existing_test_features = [feature for feature in valid_features if feature in df_test.columns]\n",
    "X_test = df_test.loc[:, existing_test_features].copy()\n",
    "missing_test_features = [feature for feature in valid_features if feature not in existing_test_features]\n",
    "for column in missing_test_features:\n",
    "    X_test[column] = np.nan\n",
    "X_test = X_test.reindex(columns=valid_features)\n",
    "\n",
    "num_feats, cat_feats = split_numeric_categorical(X_train, valid_features)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", OrdinalEncoder(columns=cat_feats, method=\"freq\"), cat_feats),\n",
    "        (\"num\", NumericImputerArbitrary(), num_feats),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "X_train_transformed_df = pd.DataFrame(\n",
    "    X_train_transformed, columns=num_feats + cat_feats, index=X_train.index\n",
    ")\n",
    "X_test_transformed_df = pd.DataFrame(\n",
    "    X_test_transformed, columns=num_feats + cat_feats, index=X_test.index\n",
    ")\n",
    "\n",
    "params_section = config[\"PARAMS\"]\n",
    "default_params = {k: _parse_value(v) for k, v in params_section.items()}\n",
    "\n",
    "print(\"default params loaded:\")\n",
    "for k, v in default_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "search_space = [\n",
    "    Integer(4, 7, name=\"max_depth\"),\n",
    "    Real(0.1, 0.2, prior=\"log-uniform\", name=\"eta\"),\n",
    "    Real(0.5, 1.0, prior=\"uniform\", name=\"subsample\"),\n",
    "    Real(0.0, 10.0, prior=\"uniform\", name=\"reg_alpha\"),\n",
    "    Real(0.0, 10.0, prior=\"uniform\", name=\"reg_lambda\"),\n",
    "    Integer(500, 3000, name=\"n_estimators\"),\n",
    "    Real(0.0, 1.0, prior=\"uniform\", name=\"colsample_bytree\"),\n",
    "    Categorical([\"hist\", \"approx\", \"gpu_hist\"], name=\"tree_method\"),\n",
    "]\n",
    "\n",
    "print(\"\n",
    "search space:\")\n",
    "for p in search_space:\n",
    "    print(p)\n",
    "\n",
    "\n",
    "@use_named_args(search_space)\n",
    "def objective(**params_opt):\n",
    "    xgb_params = default_params.copy()\n",
    "    xgb_params.update(\n",
    "        {\n",
    "            \"max_depth\": params_opt[\"max_depth\"],\n",
    "            \"eta\": params_opt[\"eta\"],\n",
    "            \"subsample\": params_opt[\"subsample\"],\n",
    "            \"reg_alpha\": params_opt[\"reg_alpha\"],\n",
    "            \"reg_lambda\": params_opt[\"reg_lambda\"],\n",
    "            \"colsample_bytree\": params_opt[\"colsample_bytree\"],\n",
    "            \"tree_method\": params_opt[\"tree_method\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dtrain_cv = build_dmatrix(X_train_transformed_df, y)\n",
    "    cv_results = xgb.cv(\n",
    "        params=xgb_params,\n",
    "        dtrain=dtrain_cv,\n",
    "        num_boost_round=params_opt[\"n_estimators\"],\n",
    "        nfold=3,\n",
    "        metrics=\"rmse\",\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=False,\n",
    "        seed=42,\n",
    "    )\n",
    "    return cv_results[\"test-rmse-mean\"].min()\n",
    "\n",
    "\n",
    "result = gp_minimize(objective, search_space, n_calls=30, random_state=42)\n",
    "\n",
    "print(\"best hyperparameters:\")\n",
    "best_param_names = [dim.name for dim in search_space]\n",
    "for name, val in zip(best_param_names, result.x):\n",
    "    print(f\"{name}: {val}\")\n",
    "\n",
    "trained_params = default_params.copy()\n",
    "for dim, val in zip(search_space, result.x):\n",
    "    if dim.name in trained_params:\n",
    "        if isinstance(dim, Integer):\n",
    "            trained_params[dim.name] = int(val)\n",
    "        else:\n",
    "            trained_params[dim.name] = val\n",
    "\n",
    "n_estimators_for_cv = int(trained_params.get(\"n_estimators\", default_params.get(\"n_estimators\", 1000)))\n",
    "\n",
    "cv_dtrain = build_dmatrix(X_train_transformed_df, y)\n",
    "cv_results = xgb.cv(\n",
    "    params=trained_params,\n",
    "    dtrain=cv_dtrain,\n",
    "    num_boost_round=n_estimators_for_cv,\n",
    "    nfold=3,\n",
    "    metrics=\"rmse\",\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=False,\n",
    "    seed=42,\n",
    ")\n",
    "best_iteration = cv_results[\"test-rmse-mean\"].idxmin()\n",
    "final_boost_rounds = int(best_iteration) + 1\n",
    "\n",
    "trained_params[\"n_estimators\"] = final_boost_rounds\n",
    "\n",
    "dtrain = build_dmatrix(X_train_transformed_df, y)\n",
    "rich_callback = RichProgressBarCallback(total_rounds=final_boost_rounds)\n",
    "\n",
    "print(\"training...\")\n",
    "model = xgb.train(\n",
    "    trained_params,\n",
    "    dtrain,\n",
    "    num_boost_round=final_boost_rounds,\n",
    "    callbacks=[rich_callback],\n",
    ")\n",
    "print(\"train complete\")\n",
    "\n",
    "dtest = build_dmatrix(X_test_transformed_df, None)\n",
    "\n",
    "preds = model.predict(dtest)\n",
    "\n",
    "results = df_test_identifiers.copy()\n",
    "results[\"prediction\"] = preds\n",
    "\n",
    "output_path = config[\"PATHS\"].get(\"output_predictions_csv\", \"xgb_test_predictions.csv\")\n",
    "results.to_csv(output_path, index=False)\n",
    "\n",
    "output = session.write_pandas(\n",
    "    results,\n",
    "    \"snowpark_jc\",\n",
    "    auto_create_table=True,\n",
    "    table_type=\"transient\",\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}